from keras.callbacks import Callback
from keras import backend as K

# https://github.com/titu1994/keras-one-cycle/blob/master/clr.py


class OneCycleLR(Callback):
    def __init__(
        self,
        num_samples,
        batch_size,
        max_lr,
        end_percentage=0.1,
        scale_percentage=None,
        maximum_momentum=0.95,
        minimum_momentum=0.85,
        verbose=True,
    ):
        """This callback implements a cyclical learning rate policy (CLR).
        This is a special case of Cyclic Learning Rates, where we have only 1 cycle.
        After the completion of 1 cycle, the learning rate will decrease rapidly to
        100th its initial lowest value.
        # Arguments:
            num_samples: Integer. Number of samples in the dataset (or scanned during one epoch).
            batch_size: Integer. Batch size during training.
            max_lr: Float. Initial learning rate. This also sets the
                starting learning rate (which will be 10x smaller than
                this), and will increase to this value during the first cycle.
            end_percentage: Float. The percentage of all the epochs of training
                that will be dedicated to sharply decreasing the learning
                rate after the completion of 1 cycle. Must be between 0 and 1.
            scale_percentage: Float or None. If float, must be between 0 and 1.
                If None, it will compute the scale_percentage automatically
                based on the `end_percentage`.
            maximum_momentum: Optional. Sets the maximum momentum (initial)
                value, which gradually drops to its lowest value in half-cycle,
                then gradually increases again to stay constant at this max value.
                Can only be used with SGD Optimizer.
            minimum_momentum: Optional. Sets the minimum momentum at the end of
                the half-cycle. Can only be used with SGD Optimizer.
            verbose: Bool. Whether to print the current learning rate after every
                epoch.
        # Reference
            - [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, weight_decay, and weight decay](https://arxiv.org/abs/1803.09820)
            - [Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates](https://arxiv.org/abs/1708.07120)
        """
        super(OneCycleLR, self).__init__()

        if end_percentage < 0.0 or end_percentage > 1.0:
            raise ValueError("`end_percentage` must be between 0 and 1")

        if scale_percentage is not None and (
            scale_percentage < 0.0 or scale_percentage > 1.0
        ):
            raise ValueError("`scale_percentage` must be between 0 and 1")

        self.initial_lr = max_lr
        self.end_percentage = end_percentage
        self.scale = (
            float(scale_percentage)
            if scale_percentage is not None
            else float(end_percentage)
        )
        self.max_momentum = maximum_momentum
        self.min_momentum = minimum_momentum
        self.verbose = verbose

        if self.max_momentum is not None and self.min_momentum is not None:
            self._update_momentum = True
        else:
            self._update_momentum = False

        self.clr_iterations = 0.0
        self.history = {}

        self.epochs = None
        self.batch_size = batch_size
        self.samples = num_samples
        self.steps = None
        self.num_iterations = None
        self.mid_cycle_id = None

    def _reset(self):
        """
        Reset the callback.
        """
        self.clr_iterations = 0.0
        self.history = {}

    def compute_lr(self):
        """
        Compute the learning rate based on which phase of the cycle it is in.
        - If in the first half of training, the learning rate gradually increases.
        - If in the second half of training, the learning rate gradually decreases.
        - If in the final `end_percentage` portion of training, the learning rate
            is quickly reduced to near 100th of the original min learning rate.
        # Returns:
            the new learning rate
        """
        if self.clr_iterations > 2 * self.mid_cycle_id:
            current_percentage = self.clr_iterations - 2 * self.mid_cycle_id
            current_percentage /= float((self.num_iterations - 2 * self.mid_cycle_id))
            new_lr = (
                self.initial_lr
                * (1.0 + (current_percentage * (1.0 - 100.0) / 100.0))
                * self.scale
            )

        elif self.clr_iterations > self.mid_cycle_id:
            current_percentage = (
                1.0 - (self.clr_iterations - self.mid_cycle_id) / self.mid_cycle_id
            )
            new_lr = (
                self.initial_lr
                * (1.0 + current_percentage * (self.scale * 100 - 1.0))
                * self.scale
            )

        else:
            current_percentage = self.clr_iterations / self.mid_cycle_id
            new_lr = (
                self.initial_lr
                * (1.0 + current_percentage * (self.scale * 100 - 1.0))
                * self.scale
            )

        if self.clr_iterations == self.num_iterations:
            self.clr_iterations = 0

        return new_lr

    def compute_momentum(self):
        """
         Compute the momentum based on which phase of the cycle it is in.
        - If in the first half of training, the momentum gradually decreases.
        - If in the second half of training, the momentum gradually increases.
        - If in the final `end_percentage` portion of training, the momentum value
            is kept constant at the maximum initial value.
        # Returns:
            the new momentum value
        """
        if self.clr_iterations > 2 * self.mid_cycle_id:
            new_momentum = self.max_momentum

        elif self.clr_iterations > self.mid_cycle_id:
            current_percentage = 1.0 - (
                (self.clr_iterations - self.mid_cycle_id) / float(self.mid_cycle_id)
            )
            new_momentum = self.max_momentum - current_percentage * (
                self.max_momentum - self.min_momentum
            )

        else:
            current_percentage = self.clr_iterations / float(self.mid_cycle_id)
            new_momentum = self.max_momentum - current_percentage * (
                self.max_momentum - self.min_momentum
            )

        return new_momentum

    def on_train_begin(self, logs={}):
        logs = logs or {}

        self.epochs = self.params["epochs"]
        # When fit generator is used
        # self.params don't have the elements 'batch_size' and 'samples'
        # self.batch_size = self.params['batch_size']
        # self.samples = self.params['samples']
        self.steps = self.params["steps"]

        if self.steps is not None:
            self.num_iterations = self.epochs * self.steps
        else:
            if (self.samples % self.batch_size) == 0:
                remainder = 0
            else:
                remainder = 1
            self.num_iterations = (
                (self.epochs + remainder) * self.samples // self.batch_size
            )

        self.mid_cycle_id = int(
            self.num_iterations * ((1.0 - self.end_percentage)) / float(2)
        )

        self._reset()
        K.set_value(self.model.optimizer.lr, self.compute_lr())

        if self._update_momentum:
            if not hasattr(self.model.optimizer, "momentum"):
                raise ValueError("Momentum can be updated only on SGD optimizer !")

            new_momentum = self.compute_momentum()
            K.set_value(self.model.optimizer.momentum, new_momentum)

    def on_batch_end(self, epoch, logs=None):
        logs = logs or {}

        self.clr_iterations += 1
        new_lr = self.compute_lr()

        self.history.setdefault("lr", []).append(K.get_value(self.model.optimizer.lr))
        K.set_value(self.model.optimizer.lr, new_lr)

        if self._update_momentum:
            if not hasattr(self.model.optimizer, "momentum"):
                raise ValueError("Momentum can be updated only on SGD optimizer !")

            new_momentum = self.compute_momentum()

            self.history.setdefault("momentum", []).append(
                K.get_value(self.model.optimizer.momentum)
            )
            K.set_value(self.model.optimizer.momentum, new_momentum)

        for k, v in logs.items():
            self.history.setdefault(k, []).append(v)

    def on_epoch_end(self, epoch, logs=None):
        if self.verbose:
            if self._update_momentum:
                print(
                    " - lr: %0.5f - momentum: %0.2f "
                    % (self.history["lr"][-1], self.history["momentum"][-1])
                )

            else:
                print(" - lr: %0.5f " % (self.history["lr"][-1]))
