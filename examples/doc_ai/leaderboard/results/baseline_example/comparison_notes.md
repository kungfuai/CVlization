# Doc AI Leaderboard - Baseline Comparison

**Run Date:** 2025-10-16 14:45:50

**Test Image:** sample.jpg (simple invoice with 2 line items)

**Models Tested:** 6

**Hardware:** NVIDIA GPU with CUDA 12.4.1

---

## Executive Summary

### üèÜ Top Performer: SURYA

- **Speed:** 7.43s (2nd fastest)
- **Accuracy:** 100%
- **Output:** Clean plain text

### Key Findings

- **3 models achieved 100% accuracy:** Surya, Nanonets-OCR, Granite-Docling
- **Models show different strengths:** Speed, formatting, structure preservation
- **Speed range:** 6.77s to 22.21s (3.3x difference)

---

## Results Summary

| Rank | Model | Time | Accuracy | Best For |
|------|-------|------|----------|----------|
| ü•á | **Surya** | 7.43s | 100% | General OCR, fast extraction |
| ü•à | **Nanonets-OCR** | 12.55s | 100% | Structured markdown, tables |
| ü•â | **Granite-Docling** | 18.87s | 100% | Layout analysis, spatial data |
| 4Ô∏è‚É£ | Moondream2 | 6.77s | ~70% | Quick previews (verify output) |
| 5Ô∏è‚É£ | Moondream3 | 14.45s | Varies* | VQA, interactive queries |
| 6Ô∏è‚É£ | Docling-Serve | 22.21s | ~82% | Basic document parsing |

*Moondream3 results may vary with prompt tuning

---

## Ground Truth

```
INVOICE
Date: October 9, 2025
Invoice #: 12345

Item         Qty    Price
Widget A     2      $10.00
Widget B     1      $25.00

Total: $45.00
```

---

## Model Outputs & Analysis

### ü•á Surya (7.43s)

```
INVOICE
Date: October 9, 2025
Invoice #: 12345
Item Qty Price
Widget A 2 $10.00
Widget B 1 $25.00
Total: $45.00
```

**Accuracy:** ‚úÖ 100%

**Strengths:**
- Fast processing
- Accurate extraction
- Clean plain text output
- Reliable and consistent

**Best For:**
- Production OCR systems
- High-volume processing
- Reliable extraction
- Real-time applications

---

### ü•à Nanonets-OCR (12.55s)

```markdown
INVOICE
Date: October 9, 2025
Invoice #: 12345

| Item | Qty | Price |
| :--- | :-- | :---- |
| Widget A | 2 | $10.00 |
| Widget B | 1 | $25.00 |

Total: $45.00
```

**Accuracy:** ‚úÖ 100%

**Strengths:**
- Beautiful markdown tables
- Excellent formatting
- Supports LaTeX equations
- Best structural preservation

**Best For:**
- Scientific papers
- Complex documents
- When presentation quality matters
- Documents with equations

---

### ü•â Granite-Docling VLM (18.87s)

```
<loc_31><loc_39><loc_199><loc_106>INVOICE <br> Date: October 9, 2025 <br> Invoice #: 12345
<loc_31><loc_130><loc_218><loc_197>Item Qty Price <br> Widget A 2 $10.00 <br> Widget B 1 $25.00
<loc_31><loc_223><loc_128><loc_245>Total: $45.00
```

**Accuracy:** ‚úÖ 100%

**Strengths:**
- Includes spatial coordinates
- Preserves layout structure
- Position-aware extraction
- Detailed location data

**Best For:**
- Layout analysis
- Position-dependent processing
- Complex document understanding
- When spatial information is needed

---

### Moondream2 (6.77s)

```
- INVOICE
- Date: October 9, 2025
- Invoice #: 12345
- Item A 2 $10.00
- Item B 1 $25.00
- Total: $45.00
```

**Accuracy:** ~70%

**What's Correct:**
- ‚úì INVOICE
- ‚úì Date: October 9, 2025
- ‚úì Invoice #: 12345
- ‚úì All numbers (2, 1, $10.00, $25.00, $45.00)
- ‚úì Total

**What's Different:**
- "Item A/B" instead of "Widget A/B" (generalized the item names)

**Strengths:**
- Very fast processing
- Clean bullet-point format
- Good for general understanding
- Readable output

**Best For:**
- Quick document previews
- When exact text matching isn't critical
- Rapid prototyping
- General document understanding

**Consideration:**
- Outputs should be verified for precise data extraction tasks
- May generalize or interpret content rather than exact transcription

---

### Moondream3 (14.45s)

```
$45.00
```

**Output:** Only extracted the total amount

**Prompt Used:** "Transcribe the text in natural reading order" (`--task ocr --ocr-mode ordered`)

**Note:**
This model is designed for Visual Question Answering (VQA) and may perform better with:
- More explicit prompts (e.g., "Extract all text from this invoice")
- Interactive queries (e.g., "What is the invoice number?")
- Targeted questions rather than general OCR

**Strengths:**
- Designed for VQA (Visual Question Answering)
- Good for conversational queries
- Can answer specific questions about content
- Interactive document exploration

**Best For:**
- Interactive document Q&A
- Targeted information extraction
- When you want to ask specific questions
- Conversational document analysis

**Consideration:**
- Default OCR mode may not extract complete documents
- Better suited for question-based queries
- Results vary significantly with prompt engineering

---

### Docling-Serve (22.21s)

```
INVOICE Date: October 9 1 2025 Invoice #: 12345

Item Qty Price Widget A 2 $10.00 Widget B 1 425.00

## Total: $45.00
```

**Accuracy:** ~82%

**What's Correct:**
- ‚úì INVOICE
- ‚úì Invoice #: 12345
- ‚úì Item, Qty, Price headers
- ‚úì Widget A (correct!)
- ‚úì 2 (quantity)
- ‚úì $10.00 (price)
- ‚úì Widget B (correct!)
- ‚úì 1 (quantity)
- ‚úì Total: $45.00

**What's Different:**
- Date: "October 9 1 2025" (minor formatting variation)
- Price: "425.00" instead of "$25.00" (currency symbol and value parsing issue)

**Strengths:**
- IBM's Docling library
- General document parsing
- Markdown output format

**Best For:**
- Basic document conversion
- When combined with post-processing
- General text extraction

**Consideration:**
- May benefit from additional validation for financial documents
- Some parsing variations in dates and currency formatting
- Post-processing can correct minor issues

---

## Recommendations by Use Case

### For Production OCR Systems

**Recommended:** Surya or Granite-Docling VLM

- Both achieved 100% accuracy
- Reliable, consistent output
- Good speed-accuracy balance

### For Scientific/Academic Papers

**Recommended:** Nanonets-OCR

- Excellent markdown table formatting
- Handles LaTeX equations
- Best structural preservation

### For Layout Analysis

**Recommended:** Granite-Docling VLM

- Spatial coordinate information
- Layout structure preservation
- Position-aware extraction

### For Quick Previews/Demos

**Recommended:** Moondream2 or Surya

- Fast processing
- Clean output format
- Good for rapid iteration

### For Interactive Document Q&A

**Recommended:** Moondream3

- Designed for visual question answering
- Good for targeted information extraction
- Works best with specific queries
- Consider prompt engineering for better results

### For Documents Requiring Validation

**Recommended:** Surya + post-processing

- High base accuracy
- Add validation layer for critical fields
- Reliable foundation

---

## Performance Characteristics

### Speed

- **Fastest:** Moondream2 (6.77s)
- **Mid-range:** Surya (7.43s), Nanonets-OCR (12.55s), Moondream3 (14.45s)
- **Slower:** Granite-Docling (18.87s), Docling-Serve (22.21s)

### Accuracy (on this test)

- **100%:** Surya, Nanonets-OCR, Granite-Docling VLM
- **~82%:** Docling-Serve (one currency parsing error)
- **~70%:** Moondream2 (generalized item names)
- **Limited:** Moondream3 (prompt-dependent, may need tuning)

### Output Format

- **Plain text:** Surya, Moondream2
- **Markdown:** Nanonets-OCR, Docling-Serve
- **Structured with coordinates:** Granite-Docling VLM
- **Varies:** Moondream3 (depends on query)

---

## Benchmark Data

```csv
model,image,time_seconds,status
docling-serve,sample.jpg,22.210416770,success
granite-docling,sample.jpg,18.871429996,success
moondream2,sample.jpg,6.770287622,success
moondream3,sample.jpg,14.459550763,success
nanonets-ocr,sample.jpg,12.550378969,success
surya,sample.jpg,7.432639683,success
```

---

## Notes on Testing

### Test Limitations

- **Single test case:** Results based on one invoice image
- **Model capabilities vary:** Each model has different design goals and strengths
- **Use case matters:** Choose based on your specific requirements
- **Context is key:** Different models excel at different tasks
- **Prompt engineering:** Some models (especially VLMs) are sensitive to prompts

### Factors to Consider When Choosing

1. **Accuracy requirements:** How precise does extraction need to be?

2. **Speed requirements:** Real-time vs batch processing?

3. **Output format needs:** Plain text, markdown, structured data?

4. **Document types:** Invoices, scientific papers, forms, etc.

5. **Use case:** OCR vs VQA vs layout analysis?

6. **Post-processing:** Can you add validation steps?

7. **Prompt tuning:** Are you willing to optimize prompts per model?

### Moondream3 Prompt Note

The Moondream3 test used the default prompt: "Transcribe the text in natural reading order"

Results may improve significantly with:
- More explicit instructions: "Extract all text from this invoice including headers, items, quantities, and prices"
- Task-specific prompts: "List all line items with their details"
- Interactive queries: Multiple targeted questions

Consider re-evaluating this model with optimized prompts for your specific use case.

### Docling-Serve Accuracy Note

~82% accuracy on this test:
- 9 out of 11 fields extracted correctly
- One date formatting variation (minor)
- One currency parsing error (moderate)

This model extracts most content correctly but may need validation for financial data.

---

## Conclusion

**For most production OCR needs:** Use **Surya** (7.43s, 100% accurate)

**For premium document quality:** Use **Nanonets-OCR** (12.55s, 100%, best formatting)

**For layout-aware processing:** Use **Granite-Docling VLM** (18.87s, 100%, spatial data)

**For rapid previews:** Consider **Moondream2** (6.77s, fast with verification)

**For interactive Q&A:** Use **Moondream3** with optimized prompts

**For general parsing:** **Docling-Serve** works with post-processing

---

*Generated by CVlization Doc AI Leaderboard - Baseline Example*
