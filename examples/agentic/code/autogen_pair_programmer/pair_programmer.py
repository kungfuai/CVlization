from __future__ import annotations

import argparse
import json
import os
import subprocess
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict

from dotenv import load_dotenv


def _load_env() -> None:
    candidates = []
    current = Path(__file__).resolve()
    for parent in current.parents:
        candidates.append(parent / ".env")
    candidates.append(Path("/cvlization_repo/.env"))
    seen = set()
    for candidate in candidates:
        if candidate in seen:
            continue
        seen.add(candidate)
        if candidate.is_file():
            load_dotenv(candidate, override=False)


_load_env()

SOLUTIONS = {
    "simple_math": """def sum_of_squares(values):
    'Return the sum of squares of `values`, validating numeric input.'
    total = 0
    for value in values:
        if not isinstance(value, (int, float)):
            raise TypeError(f"Expected numeric value, got {type(value).__name__}")
        total += value * value
    return total


def describe_segments(dataframe):
    'Placeholder function for future tasks.'
    return dataframe.describe()
""",
}

TASKS = {
    "simple_math": Path("tasks/simple_math"),
}


@dataclass
class ProviderConfig:
    provider: str
    model_id: str | None
    temperature: float


class PairProgrammingSession:
    def __init__(self, task: str, config: ProviderConfig, json_mode: bool = False) -> None:
        self.task = task
        self.config = config
        self.json_mode = json_mode
        self.task_dir = TASKS[task]
        self.conversation: list[dict[str, str]] = []
        self.logs_dir = Path("var/logs")
        self.logs_dir.mkdir(parents=True, exist_ok=True)

    # ------------------------------------------------------------------
    # Conversation helpers
    # ------------------------------------------------------------------
    def log(self, speaker: str, message: str) -> None:
        entry = {"speaker": speaker, "message": message}
        self.conversation.append(entry)
        if not self.json_mode:
            print(f"[{speaker}] {message}")

    # ------------------------------------------------------------------
    # Fallback deterministic solution
    # ------------------------------------------------------------------
    def apply_reference_solution(self) -> None:
        target = self.task_dir / "task.py"
        target.write_text(SOLUTIONS[self.task])

    # ------------------------------------------------------------------
    # AutoGen integration (best-effort, currently only for OpenAI)
    # ------------------------------------------------------------------
    def maybe_run_autogen(self) -> Dict[str, Any]:
        provider = self.config.provider
        if provider == "mock":
            self.log("System", "Running in mock mode; skipping AutoGen execution.")
            return {"used_autogen": False}

        if provider != "openai":
            self.log(
                "System",
                f"Provider '{provider}' is not yet integrated with AutoGen in this example. Falling back to mock mode.",
            )
            return {"used_autogen": False, "reason": "unsupported_provider"}

        try:
            from autogen import AssistantAgent, UserProxyAgent
        except Exception as exc:  # pragma: no cover - import failure fallback
            self.log("System", f"AutoGen import failed ({exc}); falling back to deterministic patch.")
            return {"used_autogen": False, "reason": "import_error", "error": str(exc)}

        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            self.log("System", "OPENAI_API_KEY is missing; falling back to deterministic patch.")
            return {"used_autogen": False, "reason": "missing_api_key"}

        model_id = self.config.model_id or "gpt-4o-mini"
        temperature = self.config.temperature

        config_list = [
            {
                "model": model_id,
                "api_key": api_key,
            }
        ]

        coder = AssistantAgent(
            name="Coder",
            llm_config={"config_list": config_list, "temperature": temperature},
            system_message=(
                "You are an expert Python engineer. Update the project files under the provided workspace to satisfy "
                "the test suite. Prefer small, incremental changes and summarise modifications."
            ),
        )
        reviewer = AssistantAgent(
            name="Reviewer",
            llm_config={"config_list": config_list, "temperature": temperature},
            system_message=(
                "You review diffs generated by the coder, highlight issues, and request fixes when necessary."
            ),
        )
        reviewer.register_reply(
            "conversation", coder, reply_func=lambda sender, messages, recipient, **kwargs: None
        )
        user = UserProxyAgent(
            name="Manager",
            human_input_mode="NEVER",
            code_execution_config={"work_dir": str(self.task_dir), "use_docker": False},
        )

        prompt = (
            "We need to implement the `sum_of_squares` function in tasks/simple_math/task.py so that all tests in "
            "tasks/simple_math/tests pass. Ensure non-numeric values raise TypeError. Return a short summary of "
            "your final implementation."
        )

        self.log("Manager", prompt)
        try:
            user.initiate_chat(coder, message=prompt)
            return {"used_autogen": True, "provider": provider, "model": model_id}
        except Exception as exc:  # pragma: no cover - remote errors
            self.log(
                "System",
                f"AutoGen conversation failed ({exc}). Applying deterministic patch to guarantee completion.",
            )
            return {"used_autogen": False, "reason": "autogen_failure", "error": str(exc)}

    # ------------------------------------------------------------------
    # Testing helpers
    # ------------------------------------------------------------------
    def run_pytest(self) -> subprocess.CompletedProcess:
        return subprocess.run(
            [sys.executable, "-m", "pytest", "-q", "tests"],
            cwd=str(self.task_dir),
            check=True,
            capture_output=True,
            text=True,
        )

    # ------------------------------------------------------------------
    # Session runner
    # ------------------------------------------------------------------
    def run(self) -> Dict[str, Any]:
        task_path = self.task_dir / "task.py"
        backup_path = task_path.with_suffix(".backup")
        backup_path.write_text(task_path.read_text())

        summary = {"task": self.task, "provider": self.config.provider, "tests_passed": False}
        try:
            result = self.maybe_run_autogen()
            if not result.get("used_autogen"):
                self.log("System", "Applying reference solution to ensure the task is completed.")
                self.apply_reference_solution()
            summary.update(result)

            pytest_result = self.run_pytest()
            summary["tests_passed"] = True
            summary["pytest_output"] = pytest_result.stdout if pytest_result.stdout else ""
        except subprocess.CalledProcessError as exc:
            output = (exc.stdout or "") + (exc.stderr or "")
            summary["pytest_output"] = output.strip()
        finally:
            log_path = self.logs_dir / f"session_{self.task}.json"
            log_path.write_text(json.dumps(self.conversation, indent=2))
            # restore original file for idempotent runs
            task_path.write_text(backup_path.read_text())
            backup_path.unlink(missing_ok=True)

        return summary


# ----------------------------------------------------------------------
# CLI helpers
# ----------------------------------------------------------------------

def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Run the AutoGen pair programmer example.")
    parser.add_argument("--task", type=str, default="simple_math", choices=list(TASKS.keys()))
    parser.add_argument("--llm-provider", type=str, help="Override LLM provider (default: mock)")
    parser.add_argument("--llm-model", type=str, help="Override provider-specific model ID")
    parser.add_argument("--temperature", type=float, help="Sampling temperature override")
    parser.add_argument("--json", action="store_true", help="Emit JSON summary instead of logs")
    return parser.parse_args()


def detect_provider(args: argparse.Namespace) -> ProviderConfig:
    provider = (
        args.llm_provider
        or os.getenv("PAIR_LLM_PROVIDER")
        or os.getenv("LLM_PROVIDER")
        or "mock"
    ).lower()
    model_id = args.llm_model or os.getenv("PAIR_LLM_MODEL") or os.getenv("LLM_MODEL")
    temp_value = (
        args.temperature
        if args.temperature is not None
        else os.getenv("PAIR_LLM_TEMPERATURE")
        or os.getenv("LLM_TEMPERATURE")
    )
    try:
        temperature = float(temp_value) if temp_value is not None else 0.2
    except ValueError:
        temperature = 0.2
    return ProviderConfig(provider=provider, model_id=model_id, temperature=temperature)


# ----------------------------------------------------------------------
# Main
# ----------------------------------------------------------------------

def main() -> None:
    args = parse_args()
    config = detect_provider(args)
    session = PairProgrammingSession(task=args.task, config=config, json_mode=args.json)

    summary = session.run()
    if not summary.get("tests_passed"):
        if args.json:
            print(json.dumps({"status": "failed", **summary}, indent=2))
        else:
            print("Tests failed; see pytest output or var/logs for details.")
        sys.exit(1)

    if args.json:
        print(json.dumps({"status": "success", **summary}, indent=2))
    else:
        print("\nSummary:")
        for key, value in summary.items():
            if key in {"pytest_output", "used_autogen"}:
                continue
            print(f"- {key}: {value}")


if __name__ == "__main__":
    main()
