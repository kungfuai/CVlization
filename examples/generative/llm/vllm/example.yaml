name: vllm
capability: llm
modalities: [text]
stability: experimental
resources:
  gpu: 1
  vram_gb: 8
  disk_gb: 8
image: cvl-vllm

presets:
  build:
    script: build.sh
    description: Build vLLM container (torch 2.9.1 + vLLM 0.12.0)
  predict:
    script: predict.sh
    description: "Run a local vLLM inference (no server required). Default model: allenai/Olmo-3-7B-Instruct."
  serve:
    script: serve.sh
    description: Launch OpenAI-compatible vLLM server with auto-tuning
tags: [vllm, openai-api, llm, serving, text-generation]
tasks: [text_generation, serving]

verification:
  last_verified: "2026-01-01"
  last_verification_note: "PyTorch 2.9.1+CUDA 12.8. vLLM 0.12.0 on Blackwell. Qwen2.5-0.5B verified with enforce_eager=1. FlashAttention v2 auto-detected for SM 120."
