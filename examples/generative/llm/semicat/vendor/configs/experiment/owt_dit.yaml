# @package _global_

defaults:
  - override /data: owt
  - override /model: text8_dit_lg
  - override /trainer: default

seed: 12345

trainer:
  min_epochs: 0
  min_steps: 250000
  val_check_interval: 25000
  check_val_every_n_epoch: null
  precision: bf16-mixed
  accumulate_grad_batches: 8
  gradient_clip_val: 1.0

model:
  in_shape:
    - 1024
    - 50257
  net:
    vocab_size: 50257
    length: 1024
  optimizer:
    _target_: torch.optim.AdamW
    _partial_: true
    lr: 1e-4
    weight_decay: 0.1
  scheduler:
    _target_: torch.optim.lr_scheduler.LinearLR
    _partial_: true
    start_factor: 1e-3
    total_iters: 500
  calc_nll: true
  nll_samples: 16
  compile: true

tags: ["semicat", "owt", "duo"]

data:
  batch_size: 32

logger:
  wandb:
    group: owt_dit_lg_truebb
    name: ${logger.wandb.group}-${seed}
    tags: ["semicat", "owt", "duo"]
