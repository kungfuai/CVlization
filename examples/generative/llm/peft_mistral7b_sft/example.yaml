name: peft-mistral7b-sft
capability: generative/llm
modalities:
- text
datasets:
- custom
stability: stable
resources:
  gpu: 1
  vram_gb: 24
  disk_gb: 20
presets:
  build:
    script: build.sh
    description: Build the Docker image
  train:
    script: train.sh
    description: Run supervised fine-tuning (use --sample_prompt for generation after training)
tags:
- peft
- lora
- sft
- mistral
- fine-tuning
- pytorch
- 4bit-quant
- causal-lm
tasks:
- text_generation
frameworks:
- pytorch
description: Supervised fine-tuning for causal LMs using LoRA (PEFT). Supports Mistral, Llama, Gemma, etc. Default model is Mistral 7B with 4-bit quantization (requires HuggingFace access)
image: mistral7b
verification:
  last_verified: "2026-01-02"
  last_verification_note: "PyTorch 2.9.1+CUDA 12.8 (Blackwell). Mistral 7B 4-bit LoRA SFT. Loss: 2.13â†’1.61 in 10 steps (~1.5s/step). Transformers 4.48.2, PEFT 0.14.0, bitsandbytes 0.45.5."
