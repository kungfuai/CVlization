name: gemma3-4b-sft
capability: generative/llm
modalities:
  - text
datasets:
  - mlabonne/FineTome-100k
stability: stable
resources:
  gpu: 1
  vram_gb: 8
  disk_gb: 15
image: cvlization/gemma3-4b-sft:latest
presets:
  build:
    script: build.sh
    description: Build the Docker image
  train:
    script: train.sh
    description: Fine-tune Gemma-3 4B with SFT
  test:
    script: test.sh
    description: Run smoke test (30 steps, 1000 samples)
tags:
  - unsloth
  - gemma
  - gemma-3
  - fine-tuning
  - lora
  - sft
  - pytorch
  - 4bit
tasks:
  - text_generation
  - instruction_following
frameworks:
  - pytorch
  - unsloth
  - transformers
description: Fine-tune Gemma-3 4B with Unsloth for 2x faster training. Uses LoRA/PEFT for efficient fine-tuning with train-on-responses-only masking.
verification:
  last_verified: "2026-01-02"
  last_verification_note: "PyTorch 2.9.1+CUDA 12.8 (Blackwell). Gemma3 4B 4-bit LoRA SFT. Loss: 0.96→0.54, eval_loss: 0.81→0.80 in 30 steps (~2.7min). VRAM: ~17GB."
