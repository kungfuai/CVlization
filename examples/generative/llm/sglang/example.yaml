name: sglang
capability: generative/llm
modalities: [text]
stability: experimental
resources:
  gpu: 1
  vram_gb: 8
  disk_gb: 8
image: cvl-sglang

presets:
  build:
    script: build.sh
    description: Build SGLang container (torch 2.9.1 + sglang 0.5.6.post2)
  predict:
    script: predict.sh
    description: "Run a local SGLang chat (starts server + client in one container). Default model: allenai/Olmo-3-7B-Instruct."
  serve:
    script: serve.sh
    description: Launch OpenAI-compatible SGLang server with auto-tuning
tags: [sglang, openai-api, llm, serving, text-generation]
tasks: [text_generation, serving]

verification:
  last_verified: 2025-12-16
  last_verification_note: "Build + predict verified on NVIDIA A10 (24GB) with CUDA devel base (nvcc). Models: Olmo-3-7B-Instruct (bf16, ctx=4096, mem_frac=0.9, ~14GB VRAM), Phi-4-mini-instruct (bf16, ctx=4096, mem_frac=0.9, ~7-8GB), gemma-3-1b-it (bf16, ctx=4096, mem_frac=0.9), LFM2-1.2B (bf16, ctx=4096, mem_frac=0.9, Transformers backend), Olmo-3-7B-Think (bf16, ctx=4096, mem_frac=0.9), OLMo-2-1124-7B-Instruct-preview (bf16, ctx=2048, mem_frac=0.88), internlm3-8b-instruct (bf16, ctx=2048, mem_frac=0.88), Qwen3-8B (bf16, ctx=2048, mem_frac=0.88). Hunyuan-A13B-Instruct-FP8 failed (OOM during MoE init) even with ctx=1024, mem_frac=0.85."
  status: VERIFIED
