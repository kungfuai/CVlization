# Phantom: Subject-Consistent Video Generation via Cross-Modal Alignment

<div align="center">
  
[![arXiv](https://img.shields.io/badge/arXiv%20paper-2502.11079-b31b1b.svg)](https://arxiv.org/abs/2502.11079)&nbsp;
[![project page](https://img.shields.io/badge/Project_page-More_visualizations-green)](https://phantom-video.github.io/Phantom/)&nbsp;
<a href="https://huggingface.co/bytedance-research/Phantom"><img src="https://img.shields.io/static/v1?label=%F0%9F%A4%97%20Hugging%20Face&message=Model&color=orange"></a>
</div>


> [**Phantom: Subject-Consistent Video Generation via Cross-Modal Alignment**](https://arxiv.org/abs/2502.11079)<br>
> [Lijie Liu](https://liulj13.github.io/)<sup> * </sup>, [Tianxiang Ma](https://tianxiangma.github.io/)<sup> * </sup>, [Bingchuan Li](https://scholar.google.com/citations?user=ac5Se6QAAAAJ)<sup> * &dagger;</sup>, [Zhuowei Chen](https://scholar.google.com/citations?user=ow1jGJkAAAAJ)<sup> * </sup>, [Jiawei Liu](https://scholar.google.com/citations?user=X21Fz-EAAAAJ), Gen Li, Siyu Zhou, [Qian He](https://scholar.google.com/citations?user=9rWWCgUAAAAJ), Xinglong Wu
> <br><sup> * </sup>Equal contribution,<sup> &dagger; </sup>Project lead
> <br>Intelligent Creation Team, ByteDance<br>

<p align="center">
<img src="assets/teaser.png" width=95%>
<p>

## üî• Latest News!
* May 27, 2025: üéâ We have released the Phantom-Wan-14B model, a more powerful Subject-to-Video generation model.
* Apr 23, 2025: üòä Thanks to [ComfyUI-WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper/tree/dev) for adapting ComfyUI to Phantom-Wan-1.3B. Everyone is welcome to use it!
* Apr 21, 2025: üëã Phantom-Wan is coming! We adapted the Phantom framework into the [Wan2.1](https://github.com/Wan-Video/Wan2.1) video generation model. The inference codes and checkpoint have been released.
* Apr 10, 2025: We have updated the [full version](https://arxiv.org/pdf/2502.11079v2) of the Phantom paper, which now includes more detailed descriptions of the model architecture and dataset pipeline.
* Feb 16, 2025: We proposed a novel subject-consistent video generation model, **Phantom**, and have released the [report](https://arxiv.org/pdf/2502.11079v1) publicly. For more video demos, please visit the [project page](https://phantom-video.github.io/Phantom/).


## üìë Todo List
- [x] Inference codes and Checkpoint of Phantom-Wan-1.3B 
- [x] Checkpoint of Phantom-Wan-14B
- [ ] Checkpoint of Phantom-Wan-14B Pro
- [ ] Open source Phantom-Data
- [ ] Training codes of Phantom-Wan

## üìñ Overview
Phantom is a unified video generation framework for single and multi-subject references, built on existing text-to-video and image-to-video architectures. It achieves cross-modal alignment using text-image-video triplet data by redesigning the joint text-image injection model. Additionally, it emphasizes subject consistency in human generation while enhancing ID-preserving video generation.

## ‚ö°Ô∏è Quickstart

### Installation
Clone the repo:
```sh
git clone https://github.com/Phantom-video/Phantom.git
cd Phantom
```

Install dependencies:
```sh
# Ensure torch >= 2.4.0
pip install -r requirements.txt
```

### Model Download
| Models       | Download Link                                                                                                                                           |    Notes                      |
|--------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------|
| Phantom-Wan-1.3B      | ü§ó [Huggingface](https://huggingface.co/bytedance-research/Phantom/blob/main/Phantom-Wan-1.3B.pth)   | Supports both 480P and 720P
| Phantom-Wan-14B | ü§ó [Huggingface](https://huggingface.co/bytedance-research/Phantom/tree/main) | Supports both 480P and 720P

First you need to download the 1.3B original model of Wan2.1, since our Phantom-Wan model relies on the Wan2.1 VAE and Text Encoder model. Download Wan2.1-1.3B using huggingface-cli:
``` sh
pip install "huggingface_hub[cli]"
huggingface-cli download Wan-AI/Wan2.1-T2V-1.3B --local-dir ./Wan2.1-T2V-1.3B
```

Then download the Phantom-Wan-1.3B and Phantom-Wan-14B model:
``` sh
huggingface-cli download bytedance-research/Phantom --local-dir ./Phantom-Wan-Models
```
Alternatively, you can manually download the required models and place them in the `Phantom-Wan-Models` folder.

### Run Subject-to-Video Generation

#### Phantom-Wan-1.3B

- Single-GPU inference

``` sh
python generate.py --task s2v-1.3B --size 832*480 --ckpt_dir ./Wan2.1-T2V-1.3B --phantom_ckpt ./Phantom-Wan-Models/Phantom-Wan-1.3B.pth  --ref_image "examples/ref1.png,examples/ref2.png" --prompt "ÊöñÈò≥Êº´ËøáËçâÂú∞ÔºåÊâéÁùÄÂèåÈ©¨Â∞æ„ÄÅÂ§¥Êà¥ÁªøËâ≤Ëù¥Ëù∂Áªì„ÄÅË∫´Á©øÊµÖÁªøËâ≤ËøûË°£Ë£ôÁöÑÂ∞èÂ•≥Â≠©Ëπ≤Âú®ÁõõÂºÄÁöÑÈõèËèäÊóÅ„ÄÇÂ•πË∫´ÊóÅ‰∏ÄÂè™Ê£ïÁôΩÁõ∏Èó¥ÁöÑÁãóÁãóÂêêÁùÄËàåÂ§¥ÔºåÊØõËå∏Ëå∏Â∞æÂ∑¥Ê¨¢Âø´ÊëáÊôÉ„ÄÇÂ∞èÂ•≥Â≠©Á¨ëÁùÄ‰∏æËµ∑ÈªÑÁ∫¢ÈÖçËâ≤„ÄÅÂ∏¶ÊúâËìùËâ≤ÊåâÈíÆÁöÑÁé©ÂÖ∑Áõ∏Êú∫ÔºåÂ∞ÜÂíåÁãóÁãóÁöÑÊ¨¢‰πêÁû¨Èó¥ÂÆöÊ†º„ÄÇ" --base_seed 42
```

- Multi-GPU inference using FSDP + xDiT USP

``` sh
pip install "xfuser>=0.4.1"
torchrun --nproc_per_node=8 generate.py --task s2v-1.3B --size 832*480 --ckpt_dir ./Wan2.1-T2V-1.3B --phantom_ckpt ./Phantom-Wan-Models/Phantom-Wan-1.3B.pth  --ref_image "examples/ref3.png,examples/ref4.png" --dit_fsdp --t5_fsdp --ulysses_size 4 --ring_size 2 --prompt "Â§ïÈò≥‰∏ãÔºå‰∏Ä‰ΩçÊúâÁùÄÂ∞èÈ∫¶Ëâ≤ËÇåËÇ§„ÄÅÁïôÁùÄ‰πåÈªëÈïøÂèëÁöÑÂ•≥‰∫∫Á©ø‰∏äÊúâÁùÄÂ§ßÊúµÁ´ã‰ΩìËä±ÊúµË£ÖÈ•∞„ÄÅËÇ©Ë¢ñÂ§ÑÂ∏¶ÊúâÈ£òÈÄ∏Á∫±Â∏¶ÁöÑÁ∫¢Ëâ≤Á∫±Ë£ôÔºåÊº´Ê≠•Âú®ÈáëËâ≤ÁöÑÊµ∑Êª©‰∏äÔºåÊµ∑È£éËΩªÊãÇÂ•πÁöÑÈïøÂèëÔºåÁîªÈù¢ÂîØÁæéÂä®‰∫∫„ÄÇ" --base_seed 42
```

> üí°Note: 
> * Changing `--ref_image` can achieve single reference Subject-to-Video generation or multi-reference Subject-to-Video generation. The number of reference images should be within 4.
> * To achieve the best generation results, we recommend that you describe the visual content of the reference image as accurately as possible when writing `--prompt`. For example, "examples/ref1.png" can be described as "a toy camera in yellow and red with blue buttons".
> * When the generated video is unsatisfactory, the most straightforward solution is to try changing the `--base_seed` and modifying the description in the `--prompt`.

For more inference examples, please refer to "infer.sh". You will get the following generated results:

<table style="width: 100%; border-collapse: collapse; text-align: center; border: 1px solid #ccc;">
  <tr>
    <th style="text-align: center;">
      <strong>Reference Images</strong>
    </th>
    <th style="text-align: center;">
      <strong>Generated Videos (480P)</strong>
    </th>
  </tr>

  <tr>
    <td style="text-align: center; vertical-align: middle;">
      <img src="examples/ref1.png" alt="Image 1" style="height: 180px;">
      <img src="examples/ref2.png" alt="Image 2" style="height: 180px;">
    </td>
    <td style="text-align: center; vertical-align: middle;">
      <img src="examples/ref_results/result1.gif" alt="GIF 1" style="width: 400px;">
    </td>
  </tr>

  <tr>
    <td style="text-align: center; vertical-align: middle;">
      <img src="examples/ref3.png" alt="Image 3" style="height: 180px;">
      <img src="examples/ref4.png" alt="Image 4" style="height: 180px;">
    </td>
    <td style="text-align: center; vertical-align: middle;">
      <img src="examples/ref_results/result2.gif" alt="GIF 2" style="width: 400px;">
    </td>
  </tr>

  </tr>
  <tr>
    <td style="text-align: center; vertical-align: middle;">
      <img src="examples/ref5.png" alt="Image 5" style="height: 180px;">
      <img src="examples/ref6.png" alt="Image 6" style="height: 180px;">
      <img src="examples/ref7.png" alt="Image 7" style="height: 180px;">
    </td>
    <td style="text-align: center; vertical-align: middle;">
      <img src="examples/ref_results/result3.gif" alt="GIF 3" style="width: 400px;">
    </td>
  </tr>

  <tr>
    <td style="text-align: center; vertical-align: middle;">
      <img src="examples/ref8.png" alt="Image 8" style="height: 100px;">
      <img src="examples/ref9.png" alt="Image 9" style="height: 100px;">
      <img src="examples/ref10.png" alt="Image 10" style="height: 100px;">
      <img src="examples/ref11.png" alt="Image 11" style="height: 100px;">
    </td>
    <td style="text-align: center; vertical-align: middle;">
      <img src="examples/ref_results/result4.gif" alt="GIF 4" style="width: 400px;">
    </td>
  </tr>
</table>

#### Phantom-Wan-14B

- Single-GPU inference

``` sh
python generate.py --task s2v-14B --size 832*480 --frame_num 121 --sample_fps 24 --ckpt_dir ./Wan2.1-T2V-1.3B --phantom_ckpt ./Phantom-Wan-Models --ref_image "examples/ref12.png,examples/ref13.png" --prompt "ÊâéÁùÄÂèå‰∏∏Â≠êÂ§¥ÔºåË∫´ÁùÄÁ∫¢ÈªëÈÖçËâ≤Âπ∂Â∏¶ÊúâÁÅ´ÁÑ∞Á∫πÈ•∞ÊúçÈ•∞ÔºåÈ¢àÊà¥ÈáëÈ°πÂúà„ÄÅËáÇÁº†ÈáëÊä§ËÖïÁöÑÂì™ÂêíÔºåÂíåÊúâÁùÄ‰∏ÄÂ§¥Ê∑°ËìùËâ≤Â§¥ÂèëÔºåÈ¢ùÈó¥ÊúâËìùËâ≤Âç∞ËÆ∞ÔºåË∫´ÁùÄ‰∏ÄË¢≠ÁôΩËâ≤ÈïøË¢çÁöÑÊïñ‰∏ôÔºåÂπ∂ËÇ©ÂùêÂú®ÊïôÂÆ§ÁöÑÂ∫ß‰Ωç‰∏äÔºå‰ªñ‰ª¨‰∏ìÊ≥®Âú∞ËÆ®ËÆ∫ÁùÄ‰π¶Êú¨ÂÜÖÂÆπ„ÄÇËÉåÊôØ‰∏∫ÊüîÂíåÁöÑÁÅØÂÖâÂíåÁ™óÂ§ñÂæÆÈ£éÊãÇËøáÁöÑÊ†ëÂè∂ÔºåËê•ÈÄ†Âá∫ÂÆâÈùôÂèàÂÖÖÊª°Ê¥ªÂäõÁöÑÂ≠¶‰π†Ê∞õÂõ¥„ÄÇ"
```

- Multi-GPU inference using FSDP + xDiT USP

``` sh
pip install "xfuser>=0.4.1"
torchrun --nproc_per_node=8 generate.py --task s2v-14B --size 832*480 --frame_num 121 --sample_fps 24 --ckpt_dir ./Wan2.1-T2V-1.3B --phantom_ckpt ./Phantom-Wan-Models  --ref_image "examples/ref14.png,examples/ref15.png,examples/ref16.png" --dit_fsdp --t5_fsdp --ulysses_size 8 --ring_size 1 --prompt "‰∏Ä‰ΩçÊà¥ÁùÄÈªÑËâ≤Â∏ΩÂ≠ê„ÄÅË∫´Á©øÈªÑËâ≤‰∏äË°£ÈÖçÊ£ïËâ≤ËÉåÂ∏¶ÁöÑÂç°ÈÄöËÄÅÁà∑Áà∑ÔºåÂú®Ë£ÖÈ•∞ÊúâÁ≤âËâ≤ÂíåËìùËâ≤Ê°åÊ§Ö„ÄÅÊÇ¨ÊåÇÁùÄÂΩ©Ëâ≤ÂêäÁÅØ‰∏îÊëÜÊª°ÂΩ©Ëâ≤ÂúÜÁêÉË£ÖÈ•∞ÁöÑÊ∏ÖÊñ∞Âç°ÈÄöÈ£éÊ†ºÂíñÂï°È¶ÜÈáåÔºåÁ´ØËµ∑‰∏ÄÂè™ËìùËâ≤‰∏îÂÜíÁùÄÁÉ≠Ê∞îÁöÑÂíñÂï°ÊùØÔºåÁîªÈù¢È£éÊ†ºÂç°ÈÄö„ÄÅÊ∏ÖÊñ∞„ÄÇ"
```

> üí°Note: 
> * The currently released Phantom-Wan-14B model was trained on 480P data but can also be applied to generating videos at 720P and higher resolutions, though the results may be less stable. We plan to release a version further trained on 720P data in the future.
> * The Phantom-Wan-14B model was trained on 24fps data, but it can also generate 16fps videos, similar to the native Wan2.1. However, the quality may experience a slight decline.
> * It is recommended to generate horizontal videos, as they tend to produce more stable results compared to vertical videos.

For more inference examples, please refer to "infer.sh". You will get the following generated results:

<table style="width: 100%; border-collapse: collapse; text-align: center; border: 1px solid #ccc;">
  <tr>
    <th style="text-align: center;">
      <strong>Reference Images</strong>
    </th>
    <th style="text-align: center;">
      <strong>Generated Videos (720P)</strong>
    </th>
  </tr>

  <tr>
    <td style="text-align: center; vertical-align: middle;">
      <img src="examples/ref12.png" alt="Image 1" style="height: 180px;">
      <img src="examples/ref13.png" alt="Image 2" style="height: 180px;">
    </td>
    <td style="text-align: center; vertical-align: middle;">
      <img src="examples/ref_results/result5.gif" alt="GIF 1" style="width: 400px;">
    </td>
  </tr>

  <tr>
    <td style="text-align: center; vertical-align: middle;">
      <img src="examples/ref17.png" alt="Image 3" style="height: 150px;">
      <img src="examples/ref18.png" alt="Image 4" style="height: 150px;">
    </td>
    <td style="text-align: center; vertical-align: middle;">
      <img src="examples/ref_results/result7.gif" alt="GIF 2" style="width: 400px;">
    </td>
  </tr>

  </tr>
  <tr>
    <td style="text-align: center; vertical-align: middle;">
      <img src="examples/ref14.png" alt="Image 5" style="height: 120px;">
      <img src="examples/ref15.png" alt="Image 6" style="height: 120px;">
      <img src="examples/ref16.png" alt="Image 7" style="height: 120px;">
    </td>
    <td style="text-align: center; vertical-align: middle;">
      <img src="examples/ref_results/result6.gif" alt="GIF 3" style="width: 400px;">
    </td>
  </tr>

</table>

> The GIF videos are compressed.


## Acknowledgements
We would like to express our gratitude to the SEED team for their support. Special thanks to Lu Jiang, Haoyuan Guo, Zhibei Ma, and Sen Wang for their assistance with the model and data. In addition, we are also very grateful to Siying Chen, Qingyang Li, and Wei Han for their help with the evaluation.

## ‚≠ê Citation

If Phantom is helpful, please help to ‚≠ê the repo.

If you find this project useful for your research, please consider citing our [paper](https://arxiv.org/abs/2502.11079).

### BibTeX
```bibtex
@article{liu2025phantom,
  title={Phantom: Subject-consistent video generation via cross-modal alignment},
  author={Liu, Lijie and Ma, Tianxiang and Li, Bingchuan and Chen, Zhuowei and Liu, Jiawei and Li, Gen and Zhou, Siyu and He, Qian and Wu, Xinglong},
  journal={arXiv preprint arXiv:2502.11079},
  year={2025}
}
```

## üìß Contact
If you have any comments or questions regarding this open-source project, please open a new issue or contact [Tianxiang Ma](https://tianxiangma.github.io/).