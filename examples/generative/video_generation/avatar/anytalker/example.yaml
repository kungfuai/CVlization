name: anytalker
capability: generative/video_generation
modalities:
  - vision
  - audio
  - video
datasets: []
stability: experimental
resources:
  gpu_count: 1
  vram_gb: 24
  disk_gb: 20
presets:
  build:
    script: build.sh
    description: Build the Docker image
  predict:
    script: predict.sh
    description: Generate talking video from image and audio
    path_args:
      - flag: "--image"
        type: file
      - flag: "--audio"
        type: file
      - flag: "--output"
        type: file
tasks:
  - audio_to_video
  - talking_head_generation
frameworks:
  - pytorch
tags:
  - talking-head
  - audio-driven
  - video-generation
  - diffusion
  - wan2.1
description: >
  AnyTalker generates multi-person talking videos from images and audio.
  Built on Alibaba's Wan2.1 model, it synthesizes lip-synced video with
  natural head movements and expressions.

verification:
  last_verified: "2025-12-30"
  last_verification_note: "Fixed bundled sample paths in predict.py. 40-step diffusion ~2.5 min for 3.8s audio. VRAM: ~24GB."
