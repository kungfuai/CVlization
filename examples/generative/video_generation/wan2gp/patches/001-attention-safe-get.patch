--- a/shared/attention.py
+++ b/shared/attention.py
@@ -221,7 +221,7 @@
         force_attention = "sdpa"
         if  attention_mask.dtype == torch.bfloat16 and not bfloat16_supported:
             attention_mask = attention_mask.to(torch.float16)
-    attn = offload.shared_state["_attention"] if force_attention== None else force_attention
+    attn = offload.shared_state.get("_attention", "flash") if force_attention== None else force_attention

     q,k,v = qkv_list
     qkv_list.clear()
