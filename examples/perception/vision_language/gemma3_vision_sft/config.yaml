# Gemma-3 Vision SFT Configuration
# Supports both Gemma-3 and Gemma-3N variants
# Based on:
# - https://github.com/unslothai/notebooks/blob/main/nb/Gemma3_(4B)-Vision.ipynb
# - https://github.com/unslothai/notebooks/blob/main/nb/Gemma3N_(4B)-Vision.ipynb

model:
  # Choose one of:
  # - "unsloth/gemma-3-4b-pt" (Gemma-3 pre-training checkpoint)
  # - "unsloth/gemma-3n-E4B" (Gemma-3N variant, requires timm upgrade)
  name: "unsloth/gemma-3-4b-pt"
  load_in_4bit: true  # 4-bit quantization for memory efficiency
  use_gradient_checkpointing: "unsloth"  # Memory optimization
  max_length: 2048
  chat_template: "gemma-3"  # Use "gemma-3n" for Gemma-3N model

lora:
  r: 16  # LoRA rank (use 32 for Gemma-3N for better results)
  alpha: 16  # Recommended alpha == r
  dropout: 0
  finetune_vision_layers: true  # Train vision encoder
  finetune_language_layers: true  # Train language model
  finetune_attention_modules: true  # Train attention layers
  finetune_mlp_modules: true  # Train MLP layers

dataset:
  path: "unsloth/LaTeX_OCR"  # Image-to-LaTeX dataset
  split: "train"
  max_samples: 100  # Limit for fast testing (remove for full training)
  instruction: "Write the LaTeX representation for this image."
  val_split_ratio: 0.2  # Validation split ratio (20% of data for validation)

training:
  output_dir: "outputs"
  per_device_train_batch_size: 1  # Vision models need more memory
  gradient_accumulation_steps: 4  # Effective batch size = 1 * 4 = 4
  max_grad_norm: 0.3  # Based on QLoRA paper
  warmup_ratio: 0.03
  max_steps: 30  # For quick testing; use num_train_epochs for full training
  # num_train_epochs: 2  # Uncomment for full training run
  learning_rate: 2.0e-4
  logging_steps: 1
  save_steps: 30
  val_steps: 10  # Validate every 10 steps to monitor model quality
  optim: "adamw_torch_fused"
  weight_decay: 0.001
  lr_scheduler_type: "cosine"
  seed: 3407
  test_before_training: false  # Test inference before training
  test_after_training: true  # Test inference after training
