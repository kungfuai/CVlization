name: phi-4-multimodal-instruct
capability: perception/vision_language
modalities: [vision, text, audio]
stability: stable
resources:
  gpu: 1
  vram_gb: 10
  disk_gb: 12
image: phi-4-multimodal-instruct

presets:
  build:
    script: build.sh
    description: Build the multimodal Docker image with Flash Attention 2
  predict:
    script: predict.sh
    description: Run multimodal inference (image, text, and speech)
    path_args:
      - flag: "--image"
        type: file
      - flag: "--images"
        type: file
      - flag: "--output"
        type: file
  batch_predict:
    script: batch_predict.py
    description: "Batch inference from JSONL (requires: --batch-input, --output-dir)"
    path_args:
      - flag: "--batch-input"
        type: file
      - flag: "--output-dir"
        type: dir
      - flag: "--stats-output"
        type: file
  serve:
    script: serve.sh
    description: Launch vLLM OpenAI-compatible server for Phi-4-Multimodal
tags: [phi, vlm, microsoft, ocr, vqa, captioning, reasoning, pytorch, multi-image, multimodal, speech-recognition, 128k-context, latest-2025]
tasks: [ocr, image_captioning, visual_question_answering, visual_reasoning, document_understanding, multi_image, speech_recognition, speech_translation]

verification:
  last_verified: "2025-12-30"
  last_verification_note: "Verified on Blackwell RTX PRO 6000. Fixed bundled sample handling, added examples/sample.jpg. Excellent OCR and image description quality."
