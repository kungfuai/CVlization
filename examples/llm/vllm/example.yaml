name: vllm
capability: llm
modalities: [text]
stability: experimental
resources:
  gpu: 1
  vram_gb: 8
  disk_gb: 8
image: cvl-vllm

presets:
  build:
    script: build.sh
    description: Build vLLM container (torch 2.9.1 + vLLM 0.12.0)
  predict:
    script: predict.sh
    description: Run a local vLLM inference (no server required). Default model: allenai/Olmo-3-7B-Instruct.
  serve:
    script: serve.sh
    description: Launch OpenAI-compatible vLLM server with auto-tuning
tags: [vllm, openai-api, llm, serving, text-generation]
tasks: [text_generation, serving]

verification:
  last_verified: 2025-12-16
  last_verification_note: "Verified build + local predict on NVIDIA A10 (24GB). Default model Qwen/Qwen2.5-1.5B-Instruct with max_model_len=4096, enforce_eager, gpu_memory_utilization=0.9. Peak VRAM ~3GB; output saved to outputs/result.txt."
  status: VERIFIED
